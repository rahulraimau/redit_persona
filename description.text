Reddit Persona Scraper Script Explanation
=======================================

This text file explains the `reddit_persona_scraper.py` Python script for beginners. The script takes a Reddit user's profile URL (e.g., https://www.reddit.com/user/kojied/), pulls their recent posts and comments, figures out details about them (like their interests or personality), and saves this information in a text file. Each detail includes links to the posts or comments used to figure it out.

The explanation below goes through the script line by line, describing what each part does in simple language.

---

### What the Script Does
- **Input**: You give it a Reddit user’s profile URL.
- **Action**: It grabs up to 50 of the user’s recent posts and comments using Reddit’s API (a tool that lets the script access Reddit data).
- **Analysis**: It looks at the posts/comments to guess things like what the user likes, how they sound (e.g., happy or serious), and how active they are.
- **Output**: It creates a text file (e.g., `kojied_persona.txt`) with the user’s details and links to the posts/comments that back up each detail.

---

### Requirements
- You need Python 3.8 or higher installed on your computer.
- You need to install extra tools (libraries): `praw` (to talk to Reddit), `textblob` (to check if text sounds positive or negative), and `pandas` (to organize data).
- You need Reddit API credentials (like a username and password for the script to access Reddit). You get these by signing up at https://www.reddit.com/prefs/apps.

---

### How to Run the Script
1. Save the script as `reddit_persona_scraper.py`.
2. Install the required libraries by running this in your terminal or command prompt:
   ```
   pip install praw textblob pandas
   ```
3. Edit the script to add your Reddit API credentials (explained below).
4. Run the script with:
   ```
   python reddit_persona_scraper.py
   ```
5. Type a Reddit user’s profile URL when asked.
6. Check the `reddit_personas` folder for the output text file.

---

### Line-by-Line Code Explanation

```python
import praw
```
- This line brings in the `praw` library, which lets the script connect to Reddit and get user data, like posts and comments.

```python
import re
```
- Brings in the `re` library, which helps search for specific words or patterns in text (e.g., finding "I am a programmer" in a comment).

```python
import pandas as pd
```
- Imports the `pandas` library (shortened to `pd`) to organize data, though it’s not heavily used in this script.

```python
from textblob import TextBlob
```
- Imports `TextBlob` from the `textblob` library to check if text sounds positive, negative, or neutral.

```python
from datetime import datetime
```
- Imports `datetime` to handle dates and times, like when a post was made.

```python
import uuid
```
- Imports `uuid`, which isn’t used in this script (it’s leftover from a template and can be ignored).

```python
import os
```
- Imports `os` to work with files and folders, like creating a folder to save the output.

```python
from urllib.parse import urlparse
```
- Imports `urlparse` to break down a URL (like a Reddit profile link) to get just the username.

```python
reddit = praw.Reddit(
    client_id="YOUR_CLIENT_ID",
    client_secret="YOUR_CLIENT_SECRET",
    user_agent="Reddit Persona Scraper by /u/your_username"
)
```
- Sets up a connection to Reddit using `praw`. You need to replace `YOUR_CLIENT_ID`, `YOUR_CLIENT_SECRET`, and `your_username` with your Reddit API credentials:
  - Go to https://www.reddit.com/prefs/apps, create an app, and copy the `client_id` and `client_secret`.
  - `user_agent` is a name for your script (e.g., "Reddit Persona Scraper by /u/johndoe").

```python
def extract_username_from_url(profile_url):
```
- Defines a function called `extract_username_from_url` that takes a Reddit profile URL and pulls out the username.

```python
    parsed_url = urlparse(profile_url)
```
- Breaks the URL into parts (like the path) using `urlparse`. For example, `https://www.reddit.com/user/kojied/` becomes parts like `/user/kojied`.

```python
    path = parsed_url.path.strip('/')
```
- Gets the path part of the URL (e.g., `/user/kojied`) and removes extra slashes, leaving `user/kojied`.

```python
    username = path.split('/')[-1]
```
- Splits the path on `/` and takes the last part (e.g., `kojied`) as the username.

```python
    return username
```
- Sends the username back to be used later.

```python
def scrape_user_data(username):
```
- Defines a function called `scrape_user_data` that gets posts and comments for a given username.

```python
    try:
```
- Starts a block that catches errors (like if the username doesn’t exist) so the script doesn’t crash.

```python
        redditor = reddit.redditor(username)
```
- Creates a Reddit user object for the given username, letting us access their data.

```python
        posts_data = []
        comments_data = []
```
- Creates two empty lists: one to store posts (`posts_data`) and one for comments (`comments_data`).

```python
        for submission in redditor.submissions.new(limit=50):
```
- Loops through the user’s 50 most recent posts (called submissions in Reddit’s API).

```python
            posts_data.append({
                'id': submission.id,
                'title': submission.title,
                'body': submission.selftext,
                'subreddit': submission.subreddit.display_name,
                'score': submission.score,
                'created_utc': datetime.utcfromtimestamp(submission.created_utc),
                'permalink': f"https://www.reddit.com{submission.permalink}"
            })
```
- For each post, adds a dictionary (like a data container) to `posts_data` with:
  - `id`: Unique ID of the post.
  - `title`: Post title.
  - `body`: Post text (empty for link posts).
  - `subreddit`: The subreddit name (e.g., `Python`).
  - `score`: Number of upvotes minus downvotes.
  - `created_utc`: When the post was made, converted to a readable date.
  - `permalink`: Full URL to the post.

```python
        for comment in redditor.comments.new(limit=50):
```
- Loops through the user’s 50 most recent comments.

```python
            comments_data.append({
                'id': comment.id,
                'body': comment.body,
                'subreddit': comment.subreddit.display_name,
                'score': comment.score,
                'created_utc': datetime.utcfromtimestamp(comment.created_utc),
                'permalink': f"https://www.reddit.com{comment.permalink}"
            })
```
- For each comment, adds a dictionary to `comments_data` with similar details: ID, text, subreddit, score, date, and URL.

```python
        return posts_data, comments_data
```
- Returns the lists of posts and comments to be used later.

```python
    except Exception as e:
        print(f"Error scraping data for {username}: {e}")
        return [], []
```
- If something goes wrong (e.g., user doesn’t exist), prints an error message and returns empty lists.

```python
def analyze_content(posts, comments):
```
- Defines a function called `analyze_content` that looks at posts and comments to build the user’s persona.

```python
    persona = {
        'interests': [],
        'personality_traits': [],
        'expertise_level': [],
        'demographics': [],
        'engagement_style': []
    }
    citations = []
```
- Creates a `persona` dictionary to store user details in five categories (like interests) and a `citations` list to track which posts/comments support each detail.

```python
    all_content = [(item, 'post') for item in posts] + [(item, 'comment') for item in comments]
```
- Combines posts and comments into one list, marking each item as either a “post” or “comment”.

```python
    subreddits = set(item[0]['subreddit'] for item in all_content)
```
- Gets a list of unique subreddits (e.g., `r/Python`) the user is active in.

```python
    for subreddit in subreddits:
        related_items = [(item[0]['permalink'], item[1]) for item in all_content if item[0]['subreddit'] == subreddit]
        persona['interests'].append(f"Active in r/{subreddit}")
        citations.append({
            'characteristic': f"Interest: Active in r/{subreddit}",
            'references': [item[0] for item in related_items],
            'type': 'subreddit'
        })
```
- For each subreddit:
  - Finds all posts/comments in that subreddit.
  - Adds “Active in r/subreddit” to the `interests` list.
  - Saves the URLs of those posts/comments as citations.

```python
    for item, item_type in all_content:
        text = item['body'] if item['body'] else item.get('title', '')
        if not text:
            continue
```
- Loops through all posts/comments, gets their text (body for comments, body or title for posts), and skips if there’s no text.

```python
        blob = TextBlob(text)
        sentiment = blob.sentiment.polarity
```
- Uses `TextBlob` to analyze the text’s tone (positive, negative, or neutral). `sentiment.polarity` is a number between -1 (negative) and 1 (positive).

```python
        if sentiment > 0.2:
            trait = "Positive tone"
        elif sentiment < -0.2:
            trait = "Critical or negative tone"
        else:
            trait = "Neutral tone"
```
- Decides the tone:
  - Above 0.2: Positive.
  - Below -0.2: Negative or critical.
  - Otherwise: Neutral.

```python
        if trait not in persona['personality_traits']:
            persona['personality_traits'].append(trait)
            citations.append({
                'characteristic': f"Personality: {trait}",
                'references': [item['permalink']],
                'type': item_type
            })
```
- If the tone isn’t already listed, adds it to `personality_traits` and saves the post/comment URL as a citation.

```python
    for item, item_type in all_content:
        text = item['body'] if item['body'] else item.get('title', '')
        if not text:
            continue
```
- Loops through posts/comments again to check expertise.

```python
        word_count = len(text.split())
        technical_terms = len(re.findall(r'\b(technical|expert|advanced|specialized)\b', text, re.IGNORECASE))
```
- Counts words in the text and looks for words like “technical” or “expert” to guess if the user knows a lot.

```python
        if word_count > 100 or technical_terms > 0:
            level = f"Knowledgeable in r/{item['subreddit']}"
            if level not in persona['expertise_level']:
                persona['expertise_level'].append(level)
                citations.append({
                    'characteristic': f"Expertise: {level}",
                    'references': [item['permalink']],
                    'type': item_type
                })
```
- If the text is long (over 100 words) or uses technical words, marks the user as “Knowledgeable” in that subreddit and cites the post/comment.

```python
    total_items = len(all_content)
    avg_score = sum(item[0]['score'] for item in all_content) / total_items if total_items > 0 else 0
```
- Counts total posts/comments and calculates their average score (upvotes minus downvotes).

```python
    if total_items > 30:
        style = "Highly active contributor"
    elif total_items > 10:
        style = "Moderately active contributor"
    else:
        style = "Occasional contributor"
```
- Decides how active the user is based on how many posts/comments they have:
  - Over 30: Highly active.
  - 11–30: Moderately active.
  - 10 or fewer: Occasional.

```python
    persona['engagement_style'].append(style)
    citations.append({
        'characteristic': f"Engagement: {style}",
        'references': [item[0]['permalink'] for item in all_content[:5]],
        'type': 'aggregate'
    })
```
- Adds the activity level to `engagement_style` and cites up to 5 posts/comments.

```python
    for item, item_type in all_content:
        text = item['body'] if item['body'] else item.get('title', '')
        if re.search(r"\b(I am|I'm)\s(a|an)\s(\w+)", text, re.IGNORECASE):
            match = re.search(r"\b(I am|I'm)\s(a|an)\s(\w+)", text, re.IGNORECASE).group(3)
            persona['demographics'].append(f"Self-identified as {match}")
            citations.append({
                'characteristic': f"Demographic: Self-identified as {match}",
                'references': [item['permalink']],
                'type': item_type
            })
```
- Looks for phrases like “I am a programmer” or “I’m an engineer” in the text. If found, adds the word (e.g., “programmer”) to `demographics` and cites the post/comment.

```python
    return persona, citations
```
- Returns the completed persona and citations lists.

```python
def write_persona_to_file(username, persona, citations):
```
- Defines a function to save the persona to a text file.

```python
    output_dir = "reddit_personas"
    os.makedirs(output_dir, exist_ok=True)
```
- Creates a folder called `reddit_personas` if it doesn’t exist.

```python
    filename = f"{output_dir}/{username}_persona.txt"
```
- Sets the name of the output file (e.g., `reddit_personas/kojied_persona.txt`).

```python
    with open(filename, 'w', encoding='utf-8') as f:
```
- Opens the file for writing, using `utf-8` to handle special characters.

```python
        f.write(f"User Persona for Reddit User: {username}\n")
        f.write("=" * 50 + "\n\n")
```
- Writes a title and a line of `=` signs to the file.

```python
        for category, traits in persona.items():
            if traits:
                f.write(f"{category.replace('_', ' ').title()}:\n")
```
- Loops through each persona category (e.g., `interests`). If it has items, writes the category name (e.g., “Interests”).

```python
                for trait in traits:
                    f.write(f"- {trait}\n")
```
- Writes each item in the category (e.g., “- Active in r/Python”).

```python
                    for citation in citations:
                        if citation['characteristic'].lower().startswith(category.lower()) and trait in citation['characteristic']:
                            f.write("  Citations:\n")
                            for ref in citation['references']:
                                f.write(f"    - {ref} ({citation['type'].title()})\n")
```
- For each item, finds matching citations and writes their URLs and type (e.g., “Post” or “Comment”).

```python
                f.write("\n")
```
- Adds a blank line after each category.

```python
    print(f"Persona written to {filename}")
```
- Prints a message saying the file was saved.

```python
def main():
```
- Defines the main function that runs the whole script.

```python
    profile_url = input("Enter Reddit user profile URL: ")
```
- Asks you to type a Reddit user’s profile URL.

```python
    username = extract_username_from_url(profile_url)
```
- Gets the username from the URL.

```python
    print(f"Scraping data for user: {username}")
```
- Shows which user’s data is being pulled.

```python
    posts, comments = scrape_user_data(username)
```
- Calls `scrape_user_data` to get posts and comments.

```python
    if not posts and not comments:
        print("No data retrieved. Check the username or API credentials.")
        return
```
- If no data is found, prints an error and stops.

```python
    persona, citations = analyze_content(posts, comments)
```
- Analyzes the posts/comments to build the persona.

```python
    write_persona_to_file(username, persona, citations)
```
- Saves the persona to a file.

```python
if __name__ == "__main__":
    main()
```
- Runs the `main` function when you start the script.

---

### Example Output
If you run the script with `https://www.reddit.com/user/kojied/`, it might create `reddit_personas/kojied_persona.txt` with:
```
User Persona for Reddit User: kojied
==================================================

Interests:
- Active in r/Python
  Citations:
    - https://www.reddit.com/r/Python/comments/abc123/... (Post)
- Active in r/MachineLearning
  Citations:
    - https://www.reddit.com/r/MachineLearning/comments/ghi789/... (Post)

Personality Traits:
- Positive tone
  Citations:
    - https://www.reddit.com/r/Python/comments/abc123/... (Post)

...
```

---

### Troubleshooting
- **Error about API credentials**: Make sure you added your `client_id`, `client_secret`, and `user_agent` correctly.
- **No data found**: The user might not exist or have private posts. Try another user like `kojied`.
- **SyntaxError**: Check for typos or use Python 3.8+. Open the script in a simple editor (like Notepad++ or VS Code) to spot hidden characters.
- **Library not found**: Run `pip install praw textblob pandas` again.

---

### Tips for Beginners
- **Python Basics**: The script uses lists (`[]`), dictionaries (`{}`), loops (`for`), and functions (`def`). If these are new, try a Python tutorial like Codecademy or W3Schools.
- **Reddit API**: Think of it like a library card that lets the script borrow Reddit data. You need to sign up for it first.
- **Editing the Script**: Use a text editor like VS Code to change the script. Be careful not to delete quotes (`"`) or commas (`,`).
- **Output File**: Check the `reddit_personas` folder after running the script to find the results.

If you have questions or get stuck, feel free to ask for help!

---
